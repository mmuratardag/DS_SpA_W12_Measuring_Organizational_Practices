{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adopted from\n",
    "### the notebook that <a href = \"https://github.com/cjurban\">Christopher J. Urban</a> kindly prepared. A big THANKS to him."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for conducting amortized importance-weighted variational inference for exploratory item factor analysis, which is<br>\n",
    "\" ... computationally fast even in large data sets with many latent factors.\"<br>\n",
    "\"... The IWAE approximates the marginal maximum likelihood estimator using an importance sampling technique wherein increasing the number of importance-weighted (IW) samples drawn during fitting improves the approximation, typically at the cost of decreased computational efficiency.\"\n",
    "\n",
    "Check out the publication by <a href = \"https://scholar.google.com/citations?user=Q3glDmMAAAAJ&hl=en\">Urban</a> & <a href = \"https://dbauer.web.unc.edu/\">Bauer</a>, which  is available \n",
    "<a href = \"https://arxiv.org/pdf/2001.07859.pdf\">here</a> and <a href = \"https://link.springer.com/article/10.1007/s11336-021-09748-3\">here</a>.<br>\n",
    "The reproduction material for the paper is available @ <a href = \"https://github.com/cjurban/DeepExploratoryIFA\">Urban's Github repo</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import sys\n",
    "import timeit\n",
    "from factor_analyzer import Rotator # May need to be installed; see https://pypi.org/project/factor-analyzer/\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "from pylab import *\n",
    "\n",
    "from utils import *\n",
    "from helper_layers import *\n",
    "from base_class import *\n",
    "from mirt_vae import *\n",
    "from read_data import *\n",
    "\n",
    "# Suppress scientific notation.\n",
    "np.set_printoptions(suppress = True)\n",
    "\n",
    "# Print full arrays.\n",
    "np.set_printoptions(threshold = sys.maxsize)\n",
    "\n",
    "# If CUDA is available, use it.\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "kwargs = {\"num_workers\" : 1, \"pin_memory\" : True} if cuda else {}\n",
    "\n",
    "# Set log interval.\n",
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = \"../000_data/df_efa_for_torch_nn_efa.csv\"\n",
    "\n",
    "# Full data set.\n",
    "csv_loader = torch.utils.data.DataLoader(csv_dataset(csv_file = csv_filename, \n",
    "                                                     which_split = \"full\",\n",
    "                                                     transform = to_tensor()),\n",
    "                                         batch_size = 32, shuffle = True, **kwargs)\n",
    "\n",
    "# Test data set.\n",
    "csv_test_loader = torch.utils.data.DataLoader(csv_dataset(csv_file = csv_filename,\n",
    "                                                          which_split = \"test-only\",\n",
    "                                                          test_size = 0.025,\n",
    "                                                          transform = to_tensor()),\n",
    "                                              batch_size = 32, shuffle = True, **kwargs)\n",
    "\n",
    "# Train data set.\n",
    "csv_train_loader = torch.utils.data.DataLoader(csv_dataset(csv_file = csv_filename,\n",
    "                                                           which_split = \"train-only\",\n",
    "                                                           test_size = 0.025,\n",
    "                                                           transform = to_tensor()),\n",
    "                                               batch_size = 32, shuffle = True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fitting\n",
      "Fitting completed in 200.28 seconds\n",
      "\n",
      "Computing approx. LL\n",
      "Approx. LL computed in 1.38 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds.\n",
    "seed = 666\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Initialize model.\n",
    "print(\"\\nStarting fitting\")\n",
    "start = timeit.default_timer()\n",
    "dfm_vae = MIRTVAEClass(input_dim = 406, # number of columns in binarized data matrix\n",
    "                       inference_model_dims = [136], # list of neural network hidden layer sizes\n",
    "                       latent_dim = 3,\n",
    "                       n_cats = [7] * 58, # list of number of categories for each item\n",
    "                       learning_rate = 5e-3,\n",
    "                       device = device,\n",
    "                       log_interval = log_interval,\n",
    "                       steps_anneal = 1e3)\n",
    "\n",
    "\"\"\"\n",
    "Fit model.\n",
    "Note: iw_samples can be increased to improve the approximation to the MMLE.\n",
    "As described in the paper, iw_samples = 5 seems to perform well in practice.\n",
    "\"\"\"\n",
    "dfm_vae.run_training(csv_loader, csv_test_loader, iw_samples = 5)\n",
    "stop = timeit.default_timer()\n",
    "run_time = stop - start\n",
    "print(\"Fitting completed in\", round(run_time, 2), \"seconds\")\n",
    "\n",
    "# Extract estimated loadings and intercepts.\n",
    "loadings = dfm_vae.model.loadings.weight.data.numpy()\n",
    "intercepts = dfm_vae.model.intercepts.bias.data.numpy()\n",
    "\n",
    "\"\"\"\n",
    "Rotate loadings and extract factor correlations.\n",
    "Note: In the paper I used geomin rotation, whereas here I use oblimin.\n",
    "\"\"\"\n",
    "rotator = Rotator(method = \"oblimin\")\n",
    "rot_loadings = rotator.fit_transform(loadings)\n",
    "cor_mat = rotator.phi_\n",
    "\n",
    "\"\"\"\n",
    "Compute approximate log-likelihood.\n",
    "Note: This computation can be sped up or slowed down by decreasing or increasing\n",
    "iw_samples, respectively. In the paper, I set iw_samples = 5000. \n",
    "\"\"\"\n",
    "print(\"\\nComputing approx. LL\")\n",
    "start = timeit.default_timer()\n",
    "ll = dfm_vae.bic(csv_test_loader, iw_samples = 100)[1]\n",
    "stop = timeit.default_timer()\n",
    "print(\"Approx. LL computed in\", round(stop - start, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.005863</td>\n",
       "      <td>-0.001093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001618</td>\n",
       "      <td>0.007902</td>\n",
       "      <td>0.000750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003509</td>\n",
       "      <td>-0.002859</td>\n",
       "      <td>0.001117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.000415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001714</td>\n",
       "      <td>-0.005524</td>\n",
       "      <td>-0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.001097</td>\n",
       "      <td>0.000157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.000190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002374</td>\n",
       "      <td>-0.000769</td>\n",
       "      <td>0.001418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.001186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>-0.000993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.000151</td>\n",
       "      <td>-0.003299</td>\n",
       "      <td>-0.000509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.000471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.003030</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>-0.001316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.001677</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>-0.000307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.000294</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>0.000087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000104</td>\n",
       "      <td>-0.003348</td>\n",
       "      <td>-0.000673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000085</td>\n",
       "      <td>-0.000983</td>\n",
       "      <td>-0.000575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>-0.000524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.000462</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>-0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000472</td>\n",
       "      <td>-0.000530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000178</td>\n",
       "      <td>-0.000553</td>\n",
       "      <td>-0.000743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.000748</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>-0.000201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.000646</td>\n",
       "      <td>-0.001975</td>\n",
       "      <td>-0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.000283</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>-0.000727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.001096</td>\n",
       "      <td>-0.003122</td>\n",
       "      <td>-0.001216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.000339</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>-0.001017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.001991</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.000325</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.000789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.004476</td>\n",
       "      <td>0.000830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000756</td>\n",
       "      <td>-0.001399</td>\n",
       "      <td>-0.000335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.000752</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>0.000156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000723</td>\n",
       "      <td>-0.000472</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.001794</td>\n",
       "      <td>-0.000674</td>\n",
       "      <td>-0.000437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.001438</td>\n",
       "      <td>-0.001467</td>\n",
       "      <td>0.000388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.000303</td>\n",
       "      <td>-0.000693</td>\n",
       "      <td>-0.000471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>-0.001114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>-0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.000304</td>\n",
       "      <td>-0.001721</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.000152</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>-0.000362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.000199</td>\n",
       "      <td>-0.000413</td>\n",
       "      <td>-0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000361</td>\n",
       "      <td>-0.000542</td>\n",
       "      <td>-0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>-0.000170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.001536</td>\n",
       "      <td>-0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>-0.000494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.000580</td>\n",
       "      <td>-0.003075</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.000838</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.000514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.001178</td>\n",
       "      <td>-0.000788</td>\n",
       "      <td>-0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.000303</td>\n",
       "      <td>-0.001907</td>\n",
       "      <td>-0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-0.000692</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-0.006638</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>-0.000521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>-0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-0.001756</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.000769</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>-0.000458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dim1     dim_2     dim_3\n",
       "0   0.000501  0.005863 -0.001093\n",
       "1  -0.001618  0.007902  0.000750\n",
       "2   0.003509 -0.002859  0.001117\n",
       "3   0.001359  0.001029  0.000415\n",
       "4   0.001714 -0.005524 -0.000515\n",
       "5  -0.000168 -0.001097  0.000157\n",
       "6   0.001853  0.002267  0.000190\n",
       "7   0.002374 -0.000769  0.001418\n",
       "8   0.000929  0.000626  0.001186\n",
       "9   0.001606  0.001438 -0.000993\n",
       "10 -0.000151 -0.003299 -0.000509\n",
       "11  0.000075  0.001305  0.000471\n",
       "12 -0.003030  0.001646 -0.001316\n",
       "13 -0.001677  0.000074 -0.000307\n",
       "14 -0.000294 -0.001549  0.000087\n",
       "15  0.000104 -0.003348 -0.000673\n",
       "16  0.000085 -0.000983 -0.000575\n",
       "17  0.001433  0.000307 -0.000524\n",
       "18  0.001233  0.000505  0.000546\n",
       "19 -0.000462  0.001036 -0.000075\n",
       "20  0.000024 -0.000472 -0.000530\n",
       "21  0.000178 -0.000553 -0.000743\n",
       "22 -0.000748 -0.000299  0.000266\n",
       "23  0.001737  0.001126  0.000003\n",
       "24  0.000463  0.001746 -0.000201\n",
       "25 -0.000646 -0.001975 -0.000562\n",
       "26 -0.000283  0.002163 -0.000727\n",
       "27  0.001096 -0.003122 -0.001216\n",
       "28 -0.000339  0.000998 -0.001017\n",
       "29 -0.001991  0.001955  0.000070\n",
       "30 -0.000325  0.000753  0.000413\n",
       "31  0.000976  0.000867  0.000789\n",
       "32  0.002396  0.004476  0.000830\n",
       "33  0.000756 -0.001399 -0.000335\n",
       "34 -0.000752 -0.001877  0.000156\n",
       "35  0.000723 -0.000472  0.000061\n",
       "36  0.001794 -0.000674 -0.000437\n",
       "37  0.001438 -0.001467  0.000388\n",
       "38 -0.000303 -0.000693 -0.000471\n",
       "39  0.003714  0.000475 -0.001114\n",
       "40  0.000217  0.002982 -0.000057\n",
       "41 -0.000304 -0.001721  0.000159\n",
       "42 -0.000152  0.001717 -0.000362\n",
       "43 -0.000199 -0.000413 -0.000012\n",
       "44  0.000361 -0.000542 -0.000120\n",
       "45  0.000810  0.001789 -0.000170\n",
       "46 -0.000059 -0.001536 -0.000078\n",
       "47 -0.000009  0.000712 -0.000494\n",
       "48  0.000580 -0.003075  0.000125\n",
       "49 -0.000838  0.000601  0.000514\n",
       "50  0.001178 -0.000788 -0.000013\n",
       "51 -0.000603 -0.000230  0.000052\n",
       "52  0.000303 -0.001907 -0.000038\n",
       "53 -0.000692  0.000158  0.000316\n",
       "54 -0.006638 -0.000262 -0.000521\n",
       "55  0.000270  0.000455 -0.000269\n",
       "56 -0.001756 -0.000016 -0.000236\n",
       "57 -0.000769  0.001446 -0.000458"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_loadings = normalize_loadings(rot_loadings)\n",
    "norm_loadings_df = pd.DataFrame(norm_loadings, columns = ['dim1', 'dim_2', 'dim_3'])\n",
    "norm_loadings_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
